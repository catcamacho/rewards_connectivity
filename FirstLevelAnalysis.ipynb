{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "from os.path import join\n",
    "from nipype.pipeline.engine import Workflow, Node\n",
    "from nipype.interfaces.utility import IdentityInterface, Function\n",
    "from nipype.interfaces.io import SelectFiles, DataSink, DataGrabber\n",
    "from nipype.interfaces.fsl.preprocess import FLIRT, SUSAN\n",
    "from nipype.interfaces.fsl.utils import Merge, ImageMeants\n",
    "from nipype.interfaces.fsl.model import GLM, Level1Design, FEATModel\n",
    "from nipype.algorithms.modelgen import SpecifyModel\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "# MATLAB setup - Specify path to current SPM and the MATLAB's default mode\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('~/spm12/toolbox')\n",
    "MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "\n",
    "# FSL set up- change default file output type\n",
    "from nipype.interfaces.fsl import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI')\n",
    "\n",
    "# Set study variables\n",
    "smoothing_kernel = 4\n",
    "\n",
    "analysis_home = '/Users/catcamacho/Box/LNCD_rewards_connectivity'\n",
    "#analysis_home = '/Volumes/Zeus/Cat'\n",
    "preproc_dir = analysis_home + '/subjs'\n",
    "raw_dir = analysis_home + '/subjs'\n",
    "#raw_dir = '/Volumes/Phillips/bars/APWF_bars/subjs'\n",
    "preproc_dir = analysis_home + '/proc/preprocessing'\n",
    "firstlevel_dir = analysis_home + '/proc/firstlevel'\n",
    "secondlevel_dir = analysis_home + '/proc/secondlevel'\n",
    "workflow_dir = analysis_home + '/workflows'\n",
    "template_dir = analysis_home + '/templates'\n",
    "\n",
    "MNI_template = template_dir + '/MNI152_T1_2mm_brain.nii'\n",
    "#pull subject info to iter over\n",
    "#subject_info = DataFrame.from_csv(analysis_home + '/misc/subjs.csv')\n",
    "#subjects_list = subject_info['SubjID'].tolist()\n",
    "#timepoints = subject_info['Timepoint'].tolist()\n",
    "\n",
    "subjects_list = ['10766']\n",
    "timepoints = [1]\n",
    "\n",
    "# Seed locations and seed list\n",
    "seed_dir = analysis_home + '/seeds'\n",
    "L_amyg = seed_dir + '/L_amyg_anatomical.nii'\n",
    "R_amyg = seed_dir + '/R_amyg_anatomical.nii'\n",
    "\n",
    "seeds = [L_amyg, R_amyg]\n",
    "\n",
    "TR = 1.5\n",
    "\n",
    "conditions = ['punish','reward','neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     trial    lat  Count  time_hyp    cond  stim  catch  runNum\n",
      "5        2  373.0      1       6.0  punish   cue      0       1\n",
      "6        2  373.0      1       7.5  punish  prep      0       1\n",
      "7        2  373.0      1       9.0  punish   sac      0       1\n",
      "52       8  361.0      1      78.0  punish   cue      0       1\n",
      "53       8  361.0      1      79.5  punish  prep      0       1\n",
      "54       8  361.0      1      81.0  punish   sac      0       1\n",
      "56       9  429.0      1      84.0  punish   cue      0       1\n",
      "57       9  429.0      1      85.5  punish  prep      0       1\n",
      "58       9  429.0      1      87.0  punish   sac      0       1\n",
      "84      14  337.0      1     126.0  punish   cue      0       1\n",
      "85      14  337.0      1     127.5  punish  prep      0       1\n",
      "86      14  337.0      1     129.0  punish   sac      0       1\n",
      "99      17  385.0      1     148.5  punish   cue      0       1\n",
      "100     17  385.0      1     150.0  punish  prep      0       1\n",
      "101     17  385.0      1     151.5  punish   sac      0       1\n",
      "130     20  473.0      1     195.0  punish   cue      0       1\n",
      "131     20  473.0      1     196.5  punish  prep      0       1\n",
      "132     20  473.0      1     198.0  punish   sac      0       1\n",
      "157     25  365.0      1     235.5  punish   cue      0       1\n",
      "158     25  365.0      1     237.0  punish  prep      0       1\n",
      "159     25  365.0      1     238.5  punish   sac      0       1\n",
      "185     28  365.0      1     277.5  punish   cue      0       1\n",
      "186     28  365.0      1     279.0  punish  prep      0       1\n",
      "187     28  365.0      1     280.5  punish   sac      0       1\n",
      "211     32  337.0      1     316.5  punish   cue      0       1\n",
      "212     32  337.0      1     318.0  punish  prep      0       1\n",
      "213     32  337.0      1     319.5  punish   sac      0       1\n",
      "270     41  377.0      1     405.0  punish   cue      0       1\n",
      "271     41  377.0      1     406.5  punish  prep      0       1\n",
      "272     41  377.0      1     408.0  punish   sac      0       1\n",
      "..     ...    ...    ...       ...     ...   ...    ...     ...\n",
      "93      16  389.0      1    1498.5  punish   cue      0       4\n",
      "94      16  389.0      1    1500.0  punish  prep      0       4\n",
      "95      16  389.0      1    1501.5  punish   sac      0       4\n",
      "111     20  344.0      1    1525.5  punish   cue      0       4\n",
      "112     20  344.0      1    1527.0  punish  prep      0       4\n",
      "113     20  344.0      1    1528.5  punish   sac      0       4\n",
      "118     21  429.0      1    1536.0  punish   cue      0       4\n",
      "119     21  429.0      1    1537.5  punish  prep      0       4\n",
      "120     21  429.0      1    1539.0  punish   sac      0       4\n",
      "155     25  393.0      1    1591.5  punish   cue      0       4\n",
      "156     25  393.0      1    1593.0  punish  prep      0       4\n",
      "157     25  393.0      1    1594.5  punish   sac      0       4\n",
      "159     26  325.0      1    1597.5  punish   cue      0       4\n",
      "160     26  325.0      1    1599.0  punish  prep      0       4\n",
      "161     26  325.0      1    1600.5  punish   sac      0       4\n",
      "166     27  333.0      1    1608.0  punish   cue      0       4\n",
      "167     27  333.0      1    1609.5  punish  prep      0       4\n",
      "168     27  333.0      1    1611.0  punish   sac      0       4\n",
      "187     31  289.0      1    1639.5  punish   cue      0       4\n",
      "188     31  289.0      1    1641.0  punish  prep      0       4\n",
      "189     31  289.0      1    1642.5  punish   sac      0       4\n",
      "201     33  333.0      1    1660.5  punish   cue      0       4\n",
      "202     33  333.0      1    1662.0  punish  prep      0       4\n",
      "203     33  333.0      1    1663.5  punish   sac      0       4\n",
      "258     39  293.0      1    1746.0  punish   cue      0       4\n",
      "259     39  293.0      1    1747.5  punish  prep      0       4\n",
      "260     39  293.0      1    1749.0  punish   sac      0       4\n",
      "262     40  433.0      1    1752.0  punish   cue      0       4\n",
      "263     40  433.0      1    1753.5  punish  prep      0       4\n",
      "264     40  433.0      1    1755.0  punish   sac      0       4\n",
      "\n",
      "[144 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:16: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from os.path import abspath\n",
    "from pandas import DataFrame,Series,read_table,concat\n",
    "from math import isnan\n",
    "\n",
    "run_timing_list = glob(\"/Users/catcamacho/Box/LNCD_rewards_connectivity/subjs/10766/3_20130130/timing/*score_timing.txt\")\n",
    "run_timing_list = sorted(run_timing_list)\n",
    "\n",
    "dfs = [ read_table(i,sep=' ') for i in run_timing_list ]\n",
    "k=1\n",
    "for df in dfs:\n",
    "    df.loc[:,'runNum'] = Series(k, index = df.index)\n",
    "    df.loc[:,'time_hyp'] = (k-1)*453 + df.loc[:,'time_hyp']\n",
    "    k = k+1\n",
    "df_full = concat(dfs)\n",
    "df_full = df_full.sort(['runNum','time_hyp'], ascending=[1,1])\n",
    "df_responded = df_full[df_full.loc[:,'Count'] == 1]\n",
    "df_responded = df_responded[df_responded.loc[:,'catch']==0]\n",
    "\n",
    "df_punish = df_responded[df_responded.loc[:,'cond']=='punish']\n",
    "df_reward = df_responded[df_responded.loc[:,'cond']=='reward']\n",
    "df_neutral = df_responded[df_responded.loc[:,'cond']=='neutral']\n",
    "#print(df_reward)\n",
    "print(df_punish)\n",
    "#print(df_neutral)\n",
    "#print(df_punish['time_hyp'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data handling nodes\n",
    "infosource = Node(IdentityInterface(fields=['subjid','timepoint']), \n",
    "                  name='infosource')\n",
    "infosource.iterables = [('subjid', subjects_list),('timepoint', timepoints)]\n",
    "infosource.synchronize = True\n",
    "\n",
    "#grab timing files\n",
    "time_template = {'timing':raw_dir + '/%s/%d_*/timing/*score_timing.txt'}\n",
    "timegrabber = Node(DataGrabber(sort_filelist=True,\n",
    "                               template = raw_dir + '/%s/%d_*/timing/*score_timing.txt',\n",
    "                               field_template = time_template,\n",
    "                               base_directory=raw_dir,\n",
    "                               infields=['subjid','timepoint'], \n",
    "                               template_args={'timing':[['subjid','timepoint']]}), \n",
    "                   name='timegrabber')\n",
    "\n",
    "# Grab niftis\n",
    "template = {'struct':preproc_dir + '/preproc_anat/{subjid}_t{timepoint}/reoriented_anat.nii',\n",
    "            'func': preproc_dir + '/preproc_func/{subjid}_t{timepoint}/func_filtered.nii'}\n",
    "datasource = Node(SelectFiles(template), \n",
    "                  name = 'datasource')\n",
    "\n",
    "#sink important data\n",
    "substitutions = [('_subjid_', ''),\n",
    "                 ('_timepoint_','_t')]\n",
    "datasink = Node(DataSink(substitutions=substitutions, \n",
    "                         base_directory=firstlevel_dir,\n",
    "                         container=firstlevel_dir), \n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract timing for Beta Series Method \n",
    "def timing_bars(run_timing_list, condition):\n",
    "    from os.path import abspath\n",
    "    from pandas import DataFrame,Series,read_table,concat\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    run_length = 453\n",
    "    run_timing_list = sorted(run_timing_list)\n",
    "\n",
    "    dfs = [ read_table(i,sep=' ') for i in run_timing_list ]\n",
    "    k=1\n",
    "    for df in dfs:\n",
    "        df.loc[:,'runNum'] = Series(k, index = df.index)\n",
    "        df.loc[:,'time_hyp'] = (k-1)*run_length + df.loc[:,'time_hyp']\n",
    "        k = k+1\n",
    "    df_full = concat(dfs)\n",
    "    df_full = df_full.sort(['runNum','time_hyp'], ascending=[1,1])\n",
    "    df_responded = df_full[df_full.loc[:,'Count'] == 1]\n",
    "    df_responded = df_responded[df_responded.loc[:,'catch']==0]\n",
    "\n",
    "    df_condition = df_responded[df_responded.loc[:,'cond']==condition]\n",
    "    \n",
    "    # create run number list\n",
    "    \n",
    "    \n",
    "    # create onset list\n",
    "    all_onsets = df_condition['time_hyp'].to_list()\n",
    "    \n",
    "    # create condition names\n",
    "    \n",
    "    \n",
    "    #make bunch file\n",
    "    timing = []\n",
    "    timing.insert(0,Bunch(conditions=trialNames,\n",
    "                          onsets=onsets,\n",
    "                          durations=[[1.5] for s in trialNames],\n",
    "                          amplitudes=None,\n",
    "                          tmod=None,\n",
    "                          pmod=None,\n",
    "                          regressor_names=None,\n",
    "                          regressors=None))\n",
    "    return(timing)\n",
    "\n",
    "\n",
    "# Brightness threshold should be 0.75 * the contrast between the median brain intensity and the background\n",
    "def calc_brightness_threshold(func_vol):\n",
    "    import nibabel as nib\n",
    "    from numpy import median, where\n",
    "    \n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    \n",
    "    func_nifti1 = nib.load(func)\n",
    "    func_data = func_nifti1.get_data()\n",
    "    func_data = func_data.astype(float)\n",
    "    \n",
    "    brain_values = where(func_data > 0)\n",
    "    median_thresh = median(brain_values)\n",
    "    brightness_threshold = 0.75 * median_thresh\n",
    "    return(brightness_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract timing\n",
    "pull_timing = Node(Function(input_names=['run_timing_list','condition'],\n",
    "                            output_names=['timing'],\n",
    "                            function=timing_bars), name='pull_timing')\n",
    "\n",
    "# Specify FSL model - input bunch file called subject_info\n",
    "modelspec = Node(SpecifyModel(time_repetition=TR, \n",
    "                              input_units='secs'),\n",
    "                 name='modelspec')\n",
    "\n",
    "# Generate a level 1 design\n",
    "level1design = Node(Level1Design(bases={'dgamma':{'derivs': False}},\n",
    "                                 interscan_interval=TR, # the TR\n",
    "                                 model_serial_correlations=True,\n",
    "                                 contrasts=contrasts_list), \n",
    "                    name='level1design')\n",
    "\n",
    "# Estimate Level 1\n",
    "generateModel = Node(FEATModel(), \n",
    "                     name='generateModel')\n",
    "\n",
    "# Run GLM\n",
    "extract_beta_series = Node(GLM(out_file='betas.nii'), \n",
    "                           name='extract_beta_series')\n",
    "\n",
    "# Calculate brightness threshold\n",
    "calc_bright_thresh = Node(Function(input_names=['func_vol'],\n",
    "                                   output_names=['brightness_threshold'],\n",
    "                                   function=calc_brightness_threshold), \n",
    "                          name='calc_bright_thresh')\n",
    "\n",
    "# Smooth parameter estimates- input brightness_threshold and in_file; output smoothed_file\n",
    "smooth = Node(SUSAN(fwhm=smoothing_kernel), \n",
    "              name='smooth')\n",
    "\n",
    "# Merge PEs to 1 4D volume per condition\n",
    "merge_series = Node(Merge(dimension='t'), \n",
    "                    name='merge_series')\n",
    "\n",
    "# Register to MNI space\n",
    "reg_anat2mni = Node(FLIRT(out_matrix_file='transform.mat',\n",
    "                          reference=MNI_template),\n",
    "                    name='reg_anat2mni')\n",
    "\n",
    "reg_betas2mni = Node(FLIRT(apply_xfm=True,\n",
    "                           reference=MNI_template), \n",
    "                     name='reg_betas2mni')\n",
    "\n",
    "# Extract ROI beta series: input mask and in_file, output out_file\n",
    "extract_ROI_betas = Node(ImageMeants(), name='extract_ROI_betas')\n",
    "\n",
    "# Extract beta connectivity\n",
    "beta_series_conn = Node(GLM(out_file='betas.nii',\n",
    "                            out_cope='cope.nii'), \n",
    "                        name='beta_series_conn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect the workflow\n",
    "level1workflow = Workflow(name='level1workflow')\n",
    "level1workflow.connect([(infosource, datasource,[('subjid','subjid')]),\n",
    "                        (infosource, datasource,[('timepoint','timepoint')]),\n",
    "                        (datasource, merge, [('func','in_files')]),\n",
    "                        \n",
    "                        (merge,datasink,[('merged_file','merged_runs')])\n",
    "                       ])\n",
    "level1workflow.base_dir = join(workflow_dir)\n",
    "level1workflow.write_graph(graph2use='flat')\n",
    "level1workflow.run('MultiProc', plugin_args={'n_procs': 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
